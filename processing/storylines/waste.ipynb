{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7f2faf",
   "metadata": {},
   "source": [
    "# Waste Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d91969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "import heapq\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb88388",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce601e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "onedrive = os.path.join(\"/\", \"mnt\", \"c\", \"Users\", \"eschmann\", \"OneDrive - epfl.ch\", \"Research IT\", \"Advanced Services\", \"0042 â€“ Blue City\", \"BlueCityViz\")\n",
    "SP04 = os.path.join(onedrive, \"SP04_Waste\")\n",
    "waste_dir = os.path.join(SP04, \"Waste collection points by types (density-based spatial clustering)\")\n",
    "\n",
    "sous_secteurs_filename = os.path.join(SP04, \"Lausanne Districts.gpkg\")\n",
    "Household_waste_filename = os.path.join(waste_dir, \"DI_final_clustered_centroids.csv\")\n",
    "paper_waste_filename = os.path.join(waste_dir, \"PC_final_clustered_centroids.csv\")\n",
    "glass_waste_filename = os.path.join(waste_dir, \"VE_final_clustered_centroids.csv\")\n",
    "Organic_waste_filename = os.path.join(waste_dir, \"DV_final_clustered_centroids.csv\")\n",
    "stats_filename = os.path.join(SP04, \"Waste generation per person in each neighborhood.xlsx\")\n",
    "\n",
    "sous_secteurs = gpd.read_file(sous_secteurs_filename)\n",
    "\n",
    "Household_waste = gpd.read_file(Household_waste_filename)\n",
    "paper_waste = gpd.read_file(paper_waste_filename)\n",
    "glass_waste = gpd.read_file(glass_waste_filename)\n",
    "Organic_waste = gpd.read_file(Organic_waste_filename)\n",
    "stats = pd.read_excel(stats_filename, sheet_name=\"CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dda930",
   "metadata": {},
   "outputs": [],
   "source": [
    "sous_secteurs[\"quartier_number\"] = sous_secteurs[\"RefName\"].str.extract(r'Quartier (\\d+)')[0]\n",
    "stats_mapping = stats.set_index(stats[\"Quartier\"].str.extract(r'^(\\d+)')[0])[\"Quartier\"].to_dict()\n",
    "\n",
    "sous_secteurs[\"RefName\"] = sous_secteurs[\"quartier_number\"].map(stats_mapping)\n",
    "\n",
    "sous_secteurs.drop(columns=[\"quartier_number\"], inplace=True)\n",
    "\n",
    "\n",
    "stats.rename(columns={\"household_total\": \"DI\", \"paper_total\": \"PC\", \"glass_total\": \"VE\", \"organic_total\": \"DV\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geometry(df):\n",
    "    \"\"\"\n",
    "    Create a geometry column from the longitude and latitude columns of a DataFrame.\n",
    "    \"\"\"\n",
    "    df[\"geometry\"] = gpd.points_from_xy(df[\"centroid_lon\"], df[\"centroid_lat\"])\n",
    "\n",
    "    df.drop(columns=[\"centroid_lon\", \"centroid_lat\"], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee899ada",
   "metadata": {},
   "source": [
    "## Create a map of the waste collection points in Lausanne, colored by type of waste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50620e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Household_waste = create_geometry(Household_waste)\n",
    "paper_waste = create_geometry(paper_waste)\n",
    "glass_waste = create_geometry(glass_waste)\n",
    "Organic_waste = create_geometry(Organic_waste)\n",
    "\n",
    "Household_waste[\"type\"] = \"DI\"\n",
    "paper_waste[\"type\"] = \"PC\"\n",
    "glass_waste[\"type\"] = \"VE\"\n",
    "Organic_waste[\"type\"] = \"DV\"\n",
    "\n",
    "total_waste = pd.concat([Household_waste, paper_waste, glass_waste, Organic_waste], ignore_index=True)\n",
    "\n",
    "total_waste = gpd.GeoDataFrame(total_waste, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "total_waste.to_crs(sous_secteurs.crs, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603da98",
   "metadata": {},
   "source": [
    "## Join the waste collection points with the sous-secteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c40509",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_waste[\"quartier\"] = total_waste.sjoin(sous_secteurs[[\"RefName\", \"geometry\"]], how=\"left\", predicate=\"within\")[\"RefName\"]\n",
    "total_waste.dropna(subset=\"quartier\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a436474",
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_types = total_waste[\"type\"].unique()\n",
    "Quartiers = total_waste[\"quartier\"].unique()\n",
    "\n",
    "for wt in waste_types:\n",
    "    for q in Quartiers:\n",
    "        count = total_waste[(total_waste[\"type\"] == wt) & (total_waste[\"quartier\"] == q)].shape[0]\n",
    "        total_waste.loc[(total_waste[\"type\"] == wt) & (total_waste[\"quartier\"] == q), \"amount\"] = stats.loc[(stats[\"Quartier\"] == q)][wt].values[0] / count\n",
    "\n",
    "total_waste[\"amount_month\"] = total_waste[\"amount\"] / 12\n",
    "total_waste[\"amount_week\"] = total_waste[\"amount\"] / 52\n",
    "total_waste[\"amount_day\"] = total_waste[\"amount\"] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(total_waste[\"amount_day\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104d38f",
   "metadata": {},
   "source": [
    "## Group the waste collection points by cluster of max 100kg of waste per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regroup_cluster_dumb(waste, map_centroid, max_amount_per_cluster=500):\n",
    "    \"\"\"\n",
    "    regroup cluster based on their position, type of waste and the amount of waste collected per week\n",
    "    \"\"\"\n",
    "    waste[\"cluster\"] = None\n",
    "    waste[\"angle\"] = waste.geometry.apply(lambda x: math.atan2(x.y - map_centroid.y, x.x - map_centroid.x))\n",
    "    waste = waste.sort_values([\"type\", \"angle\"]).reset_index(drop=True)\n",
    "    \n",
    "    current_weight = 0\n",
    "    current_cluster = 0\n",
    "    current_type = waste[\"type\"].iloc[0]\n",
    "\n",
    "    for idx, cw in waste.iterrows():\n",
    "        \n",
    "        if current_weight + cw[\"amount_week\"] > max_amount_per_cluster or cw[\"type\"] != current_type:\n",
    "            current_cluster += 1\n",
    "            current_weight = 0\n",
    "            current_type = cw[\"type\"]\n",
    "        \n",
    "        current_weight += cw[\"amount_week\"]\n",
    "        waste.loc[idx, \"cluster\"] = current_cluster\n",
    "    \n",
    "    waste.drop(columns=[\"angle\"], inplace=True)\n",
    "\n",
    "    return waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb849ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capacitated_spatial_clustering(gdf, capacity, amount_col=\"amount\",\n",
    "                                   k_neighbors=10,\n",
    "                                   local_search_iterations=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame with Point geometries\n",
    "    capacity : max weight per cluster\n",
    "    amount_col : column containing weight\n",
    "    k_neighbors : neighbor search size during merging\n",
    "    local_search_iterations : local improvement loops\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame with new column 'cluster'\n",
    "    \"\"\"\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    coords = np.array([[geom.x, geom.y] for geom in gdf.geometry])\n",
    "    weights = gdf[amount_col].values\n",
    "\n",
    "    total_weight = weights.sum()\n",
    "    lower_bound = ceil(total_weight / capacity)\n",
    "    k0 = 2 * lower_bound \n",
    "\n",
    "    # Preclustering (k-means)\n",
    "    kmeans = KMeans(n_clusters=k0, random_state=0, n_init=\"auto\")\n",
    "    micro_labels = kmeans.fit_predict(coords)\n",
    "\n",
    "    clusters = {}\n",
    "    for i in range(k0):\n",
    "        idx = np.where(micro_labels == i)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        cluster_weight = weights[idx].sum()\n",
    "        centroid = coords[idx].mean(axis=0)\n",
    "\n",
    "        clusters[i] = {\n",
    "            \"points\": set(idx),\n",
    "            \"weight\": cluster_weight,\n",
    "            \"centroid\": centroid\n",
    "        }\n",
    "\n",
    "    # Greedy merging\n",
    "    clusters = greedy_merge(clusters, capacity, k_neighbors)\n",
    "\n",
    "    # Local improvement\n",
    "    clusters = local_improvement(clusters, coords, weights,\n",
    "                                  capacity,\n",
    "                                  iterations=local_search_iterations)\n",
    "\n",
    "    # Assign final labels\n",
    "    final_labels = np.empty(len(gdf), dtype=int)\n",
    "    for cid, cluster in enumerate(clusters.values()):\n",
    "        for idx in cluster[\"points\"]:\n",
    "            final_labels[idx] = cid\n",
    "\n",
    "    gdf[\"cluster\"] = final_labels\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Greedy merging phase\n",
    "\n",
    "def greedy_merge(clusters, capacity, k_neighbors):\n",
    "    cluster_ids = list(clusters.keys())\n",
    "    centroids = np.array([clusters[c][\"centroid\"] for c in cluster_ids])\n",
    "\n",
    "    tree = KDTree(centroids)\n",
    "    heap = []\n",
    "\n",
    "    # Build candidate merge heap\n",
    "    for i, cid in enumerate(cluster_ids):\n",
    "        dists, inds = tree.query([clusters[cid][\"centroid\"]],\n",
    "                                 k=min(k_neighbors + 1, len(cluster_ids)))\n",
    "        for j in inds[0][1:]:\n",
    "            cid2 = cluster_ids[j]\n",
    "            dist = np.linalg.norm(\n",
    "                clusters[cid][\"centroid\"] -\n",
    "                clusters[cid2][\"centroid\"]\n",
    "            )\n",
    "            heapq.heappush(heap, (dist, cid, cid2))\n",
    "\n",
    "    active = set(cluster_ids)\n",
    "\n",
    "    while heap:\n",
    "        dist, c1, c2 = heapq.heappop(heap)\n",
    "\n",
    "        if c1 not in active or c2 not in active:\n",
    "            continue\n",
    "\n",
    "        if clusters[c1][\"weight\"] + clusters[c2][\"weight\"] <= capacity:\n",
    "\n",
    "            # Merge\n",
    "            new_points = clusters[c1][\"points\"] | clusters[c2][\"points\"]\n",
    "            new_weight = clusters[c1][\"weight\"] + clusters[c2][\"weight\"]\n",
    "\n",
    "            new_centroid = np.mean(\n",
    "                np.vstack([clusters[c1][\"centroid\"],\n",
    "                           clusters[c2][\"centroid\"]]),\n",
    "                axis=0\n",
    "            )\n",
    "\n",
    "            clusters[c1] = {\n",
    "                \"points\": new_points,\n",
    "                \"weight\": new_weight,\n",
    "                \"centroid\": new_centroid\n",
    "            }\n",
    "\n",
    "            active.remove(c2)\n",
    "            del clusters[c2]\n",
    "\n",
    "    # Reindex clusters\n",
    "    new_clusters = {}\n",
    "    for i, cid in enumerate(active):\n",
    "        new_clusters[i] = clusters[cid]\n",
    "\n",
    "    return new_clusters\n",
    "\n",
    "\n",
    "# Local improvement phase\n",
    "\n",
    "def local_improvement(clusters, coords, weights,\n",
    "                      capacity, iterations=5):\n",
    "\n",
    "    for _ in range(iterations):\n",
    "\n",
    "        centroids = {\n",
    "            cid: cluster[\"centroid\"]\n",
    "            for cid, cluster in clusters.items()\n",
    "        }\n",
    "\n",
    "        for cid, cluster in clusters.items():\n",
    "\n",
    "            for p in list(cluster[\"points\"]):\n",
    "\n",
    "                best_target = None\n",
    "                best_delta = 0\n",
    "\n",
    "                for other_cid, other_cluster in clusters.items():\n",
    "                    if other_cid == cid:\n",
    "                        continue\n",
    "\n",
    "                    if other_cluster[\"weight\"] + weights[p] > capacity:\n",
    "                        continue\n",
    "\n",
    "                    old_dist = np.linalg.norm(\n",
    "                        coords[p] - centroids[cid]\n",
    "                    )\n",
    "                    new_dist = np.linalg.norm(\n",
    "                        coords[p] - centroids[other_cid]\n",
    "                    )\n",
    "\n",
    "                    delta = new_dist - old_dist\n",
    "\n",
    "                    if delta < best_delta:\n",
    "                        best_delta = delta\n",
    "                        best_target = other_cid\n",
    "\n",
    "                if best_target is not None:\n",
    "                    # Move point\n",
    "                    clusters[cid][\"points\"].remove(p)\n",
    "                    clusters[cid][\"weight\"] -= weights[p]\n",
    "\n",
    "                    clusters[best_target][\"points\"].add(p)\n",
    "                    clusters[best_target][\"weight\"] += weights[p]\n",
    "\n",
    "        # Recompute centroids\n",
    "        for cid, cluster in clusters.items():\n",
    "            pts = list(cluster[\"points\"])\n",
    "            if pts:\n",
    "                cluster[\"centroid\"] = coords[pts].mean(axis=0)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_centroid = sous_secteurs.unary_union.centroid\n",
    "#\n",
    "#total_waste = regroup_cluster_dumb(total_waste, map_centroid)\n",
    "\n",
    "final_waste = gpd.GeoDataFrame()\n",
    "\n",
    "for idx, wt in enumerate(waste_types):\n",
    "    clusters = capacitated_spatial_clustering(total_waste[total_waste[\"type\"] == wt], capacity=4000, amount_col=\"amount_week\")\n",
    "    clusters[\"cluster\"] = clusters[\"cluster\"] + idx * 100  # Offset cluster IDs to avoid conflicts\n",
    "    #add all new clusters to final_waste\n",
    "    final_waste = pd.concat([final_waste, clusters], ignore_index=True)\n",
    "\n",
    "final_waste.to_file(os.path.join(\".\", \"final_waste_clusters.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c471e3",
   "metadata": {},
   "source": [
    "## Plot the clusters on the map, colored by cluster and sized by the amount of waste collected per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "\n",
    "sous_secteurs.plot(ax=ax, color='lightgray', edgecolor='black', alpha=0.5)\n",
    "\n",
    "final_waste.plot(\n",
    "    ax=ax,\n",
    "    column='cluster',\n",
    "    markersize=final_waste['amount_week'] / 10,  \n",
    "    cmap='tab20',\n",
    "    legend=False,\n",
    "    alpha=0.7,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Waste Collection Points by Cluster\\n(sized by weekly waste amount)', fontsize=16)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluecity-viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
